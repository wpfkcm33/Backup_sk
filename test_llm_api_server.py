#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ìš© LLM API ì„œë²„
ì‹¤ì œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ API ìš”ì²­ì— ì‘ë‹µí•˜ëŠ” í…ŒìŠ¤íŠ¸ ì„œë²„
"""

from flask import Flask, request, jsonify
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import logging
import time
import os
import threading

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# ê¸€ë¡œë²Œ ë³€ìˆ˜
llm = None
tokenizer = None
model_loaded = False

def load_model():
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global llm, tokenizer, model_loaded
    
    try:
        # ëª¨ë¸ ì„¤ì • - LLMManagerì™€ ë™ì¼í•œ ì„¤ì • ì‚¬ìš©
        model_name = "Qwen/Qwen3-32B-AWQ"  # Qwen32B ëª¨ë¸ ì‚¬ìš©
        
        logger.info(f"Loading model: {model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¨¼ì € ë¡œë“œ
        logger.info("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        logger.info("Tokenizer loaded successfully.")
        
        # vLLM ëª¨ë¸ ë¡œë“œ - LLMManagerì™€ ë™ì¼í•œ ì„¤ì •
        logger.info("Loading vLLM model...")
        llm = LLM(
            model=model_name,
            tensor_parallel_size=2,  # 2ê°œ GPU ì‚¬ìš© (Configì™€ ë™ì¼)
            gpu_memory_utilization=0.85,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì ˆ
            max_model_len=16384,  # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            dtype="auto",  # ìë™ ë°ì´í„° íƒ€ì…
            enforce_eager=False,  # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ TorchScript ì‚¬ìš©
            trust_remote_code=True  # ì›ê²© ëª¨ë¸ ì½”ë“œ ì‹ ë¢°
        )
        
        model_loaded = True
        logger.info("Model and tokenizer loaded successfully.")
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        model_loaded = False

# ì„œë²„ ì‹œì‘ ì‹œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ë¡œë“œ
threading.Thread(target=load_model, daemon=True).start()

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "status": "healthy" if model_loaded else "loading",
        "model_loaded": model_loaded,
        "timestamp": time.time()
    })

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """OpenAI í˜¸í™˜ ì±„íŒ… ì™„ì„± API"""
    
    # ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
    if not model_loaded:
        return jsonify({
            "error": {
                "message": "Model is still loading. Please wait and try again.",
                "type": "model_loading_error",
                "code": "model_not_ready"
            }
        }), 503
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = request.get_json()
        
        if not data:
            return jsonify({
                "error": {
                    "message": "Invalid JSON in request body",
                    "type": "invalid_request_error"
                }
            }), 400
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        messages = data.get('messages', [])
        if not messages:
            return jsonify({
                "error": {
                    "message": "Messages field is required",
                    "type": "invalid_request_error"
                }
            }), 400
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        model_name = data.get('model', 'Qwen3-32B')
        max_tokens = data.get('max_tokens', 2048)
        temperature = data.get('temperature', 0.6)
        top_p = data.get('top_p', 0.95)
        
        logger.info(f"Processing chat completion request with {len(messages)} messages")
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        if not any(msg.get('role') == 'system' for msg in messages):
            system_message = {
                "role": "system", 
                "content": "You are a helpful assistant."
            }
            messages = [system_message] + messages
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš© - LLMManagerì™€ ë™ì¼í•œ ë°©ì‹
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # QwQ ëª¨ë¸ íŠ¹ì„± ë¹„í™œì„±í™”
        )
        
        logger.info("Generating response...")
        generation_start_time = time.time()
        
        # vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì • - LLMManagerì™€ ë™ì¼í•œ ì„¤ì •
        sampling_params = SamplingParams(
            temperature=temperature,
            top_p=top_p,
            top_k=30,  # LLMManagerì™€ ë™ì¼
            max_tokens=max_tokens,
            presence_penalty=0.0,  # LLMManagerì™€ ë™ì¼
            frequency_penalty=0.0  # LLMManagerì™€ ë™ì¼
        )
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        generations = llm.generate(text, sampling_params)
        output = generations[0].outputs[0].text
        
        generation_time = time.time() - generation_start_time
        logger.info(f"Generation completed in {generation_time:.2f} seconds")
        
        # OpenAI í˜•ì‹ì˜ ì‘ë‹µ ìƒì„±
        response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_name,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": output
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(text.split()),  # ëŒ€ëµì ì¸ í† í° ìˆ˜
                "completion_tokens": len(output.split()),  # ëŒ€ëµì ì¸ í† í° ìˆ˜
                "total_tokens": len(text.split()) + len(output.split())
            }
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error in chat completion: {e}")
        return jsonify({
            "error": {
                "message": f"Internal server error: {str(e)}",
                "type": "internal_error"
            }
        }), 500

@app.route('/v1/models', methods=['GET'])
def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return jsonify({
        "object": "list",
        "data": [
            {
                "id": "Qwen2.5-32B-Instruct",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "test-server"
            },
            {
                "id": "Qwen3-32B",
                "object": "model", 
                "created": int(time.time()),
                "owned_by": "test-server"
            }
        ]
    })

@app.errorhandler(404)
def not_found(error):
    """404 ì—ëŸ¬ í•¸ë“¤ëŸ¬"""
    return jsonify({
        "error": {
            "message": "Not found",
            "type": "invalid_request_error"
        }
    }), 404

@app.errorhandler(500)
def internal_error(error):
    """500 ì—ëŸ¬ í•¸ë“¤ëŸ¬"""
    return jsonify({
        "error": {
            "message": "Internal server error",
            "type": "internal_error"
        }
    }), 500

if __name__ == '__main__':
    print("ğŸš€ í…ŒìŠ¤íŠ¸ìš© LLM API ì„œë²„ ì‹œì‘")
    print("=" * 50)
    print("ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
    print("   GET  /health - í—¬ìŠ¤ ì²´í¬")
    print("   GET  /v1/models - ëª¨ë¸ ëª©ë¡")
    print("   POST /v1/chat/completions - ì±„íŒ… ì™„ì„±")
    print("=" * 50)
    print("âš ï¸  ëª¨ë¸ ë¡œë”©ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("   /health ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¡œë”© ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    print("=" * 50)
    
    # ê°œë°œ ì„œë²„ ì‹¤í–‰ (ìš´ì˜ì—ì„œëŠ” gunicorn ë“± ì‚¬ìš© ê¶Œì¥)
    app.run(
        host='0.0.0.0',  # ëª¨ë“  ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
        port=8000,       # í¬íŠ¸ 8000 ì‚¬ìš©
        debug=False,     # ë””ë²„ê·¸ ëª¨ë“œ ë¹„í™œì„±í™” (ëª¨ë¸ ë¡œë”© ì•ˆì •ì„±)
        threaded=True    # ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì›
    )